<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入浅出全面解析 RDMA]]></title>
    <url>%2F2019%2F02%2F14%2F%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-RDMA%2F</url>
    <content type="text"><![CDATA[RDMA(RemoteDirect Memory Access)技术全称远程直接内存访问，就是为了解决网络传输中服务器端数据处理的延迟而产生的。它将数据直接从一台计算机的内存传输到另一台计算机，无需双方操作系统的介入。这允许高吞吐、低延迟的网络通信，尤其适合在大规模并行计算机集群中使用。RDMA通过网络把资料直接传入计算机的存储区，将数据从一个系统快速移动到远程系统存储器中，而不对操作系统造成任何影响，这样就不需要用到多少计算机的处理能力。它消除了外部存储器复制和上下文切换的开销，因而能解放内存带宽和CPU周期用于改进应用系统性能。 本次详解我们从三个方面详细介绍RDMA：RDMA背景、RDMA相关工作、RDMA技术详解。 一、背景介绍 1.1 传统TCP/IP通信模式 传统的TCP/IP网络通信，数据需要通过用户空间发送到远程机器的用户空间。数据发送方需要讲数据从用户应用空间Buffer复制到内核空间的Socket Buffer中。然后Kernel空间中添加数据包头，进行数据封装。通过一系列多层网络协议的数据包处理工作，这些协议包括传输控制协议（TCP）、用户数据报协议（UDP）、互联网协议（IP）以及互联网控制消息协议（ICMP）等。数据才被Push到NIC网卡中的Buffer进行网络传输。消息接受方接受从远程机器发送的数据包后，要将数据包从NIC buffer中复制数据到Socket Buffer。然后经过一些列的多层网络协议进行数据包的解析工作。解析后的数据被复制到相应位置的用户应用空间Buffer。这个时候再进行系统上下文切换，用户应用程序才被调用。以上就是传统的TCP/IP协议层的工作。 如今随着社会的发展，我们希望更快和更轻量级的网络通信。 1.2 通信网络定义 计算机网络通信中最重要两个衡量指标主要是指高带宽和低延迟。通信延迟主要是指：处理延迟和网络传输延迟。处理延迟开销指的就是消息在发送和接收阶段的处理时间。网络传输延迟指的就是消息在发送和接收方的网络传输时延。如果网络通信状况很好的情况下，网络基本上可以 达到高带宽和低延迟。 1.3 当今网络现状 当今随着计算机网络的发展。消息通信主要分为两类消息，一类是Large messages，在这类消息通信中，网络传输延迟占整个通信中的主导位置。还有一类消息是Small messages，在这类消息通信中，消息发送端和接受端的处理开销占整个通信的主导地位。然而在现实计算机网络中的通信场景中，主要是以发送小消息为主。所有说发送消息和接受消息的处理开销占整个通信的主导的地位。具体来说，处理开销指的是buffer管理、在不同内存空间中消息复制、以及消息发送完成后的系统中断。 1.4 传统TCP/IP存在的问题 传统的TPC/IP存在的问题主要是指I/O bottleneck瓶颈问题。在高速网络条件下与网络I/O相关的主机处理的高开销限制了可以在机器之间发送的带宽。这里感兴趣的高额开销是数据移动操作和复制操作。具体来讲，主要是传统的TCP/IP网络通信是通过内核发送消息。Messaging passing through kernel这种方式会导致很低的性能和很低的灵活性。性能低下的原因主要是由于网络通信通过内核传递，这种通信方式存在的很高的数据移动和数据复制的开销。并且现如今内存带宽性相较如CPU带宽和网络带宽有着很大的差异。很低的灵活性的原因主要是所有网络通信协议通过内核传递，这种方式很难去支持新的网络协议和新的消息通信协议以及发送和接收接口。 二、相关工作 高性能网络通信历史发展主要有以下四个方面：TCP Offloading Engine（TOE）、User-Net Networking(U-Net)、Virtual interface Architecture（VIA）、Remote Direct Memroy Access(RDMA)。U-Net是第一个跨过内核网络通信的模式之一。VIA首次提出了标准化user-level的网络通信模式，其次它组合了U-Net接口和远程DMA设备。RDMA就是现代化高性能网络通信技术。 2.1 TCP Offloading Engine 在主机通过网络进行通信的过程中，主机处理器需要耗费大量资源进行多层网络协议的数据包处理工作，这些协议包括传输控制协议（TCP）、用户数据报协议（UDP）、互联网协议（IP）以及互联网控制消息协议（ICMP）等。由于CPU需要进行繁重的封装网络数据包协议，为了将占用的这部分主机处理器资源解放出来专注于其他应用，人们发明了TOE（TCP/IP Offloading Engine）技术，将上述主机处理器的工作转移到网卡上。 这种技术需要特定网络接口-网卡支持这种Offloading操作。这种特定网卡能够支持封装多层网络协议的数据包，这个功能常见于高速以太网接口上，如吉比特以太网（GbE）或10吉比特以太网（10GbE）。 2.2 User-Net Networking(U-Net) U-Net的设计目标是将协议处理部分移动到用户空间去处理。这种方式避免了用户空间将数据移动和复制到内核空间的开销。它的设计宗旨就是移动整个协议栈到用户空间中去，并且从数据通信路径中彻底删除内核。这种设计带来了高性能的提升和高灵活性的提升。 U-Net的virtual NI 为每个进程提供了一种拥有网络接口的错觉，内核接口只涉及到连接步骤。传统上的网络，内核控制整个网络通信，所有的通信都需要通过内核来传递。U-Net应用程序可以通过MUX直接访问网络，应用程序通过MUX直接访问内核，而不需要将数据移动和复制到内核空间中去。 三、RDMA详解 RDMA(Remote Direct Memory Access)技术全称远程直接内存访问，就是为了解决网络传输中服务器端数据处理的延迟而产生的。RDMA通过网络把资料直接传入计算机的存储区，将数据从一个系统快速移动到远程系统存储器中，而不对操作系统造成任何影响，这样就不需要用到多少计算机的处理功能。它消除了外部存储器复制和上下文切换的开销，因而能解放内存带宽和CPU周期用于改进应用系统性能。 RDMA主要有以下三个特性：1.Low-Latency 2.Low CPU overhead 3. high bandwidth 3.1 RDMA 简介Remote：数据通过网络与远程机器间进行数据传输 Direct：没有内核的参与，有关发送传输的所有内容都卸载到网卡上 Memory：在用户空间虚拟内存与RNIC网卡直接进行数据传输不涉及到系统内核，没有额外的数据移动和复制 Access：send、receive、read、write、atomic操作 3.2 RDMA基本概念 RDMA有两种基本操作。 Memory verbs: 包括RDMA read、write和atomic操作。这些操作指定远程地址进行操作并且绕过接收者的CPU。Messaging verbs:包括RDMA send、receive操作。这些动作涉及响应者的CPU，发送的数据被写入由响应者的CPU先前发布的接受所指定的地址。 RDMA传输分为可靠和不可靠的，并且可以连接和不连接的（数据报）。凭借可靠的传输，NIC使用确认来保证消息的按序传送。不可靠的传输不提供这样的保证。然而，像InfiniBand这样的现代RDMA实现使用了一个无损链路层，它可以防止使用链路层流量控制的基于拥塞的损失[1]，以及使用链路层重传的基于位错误的损失[8]。因此，不可靠的传输很少会丢弃数据包。 目前的RDMA硬件提供一种数据报传输：不可靠的数据报（UD），并且不支持memory verbs。 3.3 RDMA三种不同的硬件实现 目前RDMA有三种不同的硬件实现。分别是InfiniBand、iWarp（internet Wide Area RDMA Protocol）、RoCE(RDMA over Converged Ethernet)。 目前，大致有三类RDMA网络，分别是Infiniband、RoCE、iWARP。其中，Infiniband是一种专为RDMA设计的网络，从硬件级别保证可靠传输 ， 而RoCE 和 iWARP都是基于以太网的RDMA技术，支持相应的verbs接口，如图1所示。从图中不难发现，RoCE协议存在RoCEv1和RoCEv2两个版本，主要区别RoCEv1是基于以太网链路层实现的RDMA协议(交换机需要支持PFC等流控技术，在物理层保证可靠传输)，而RoCEv2是以太网TCP/IP协议中UDP层实现。从性能上，很明显Infiniband网络最好，但网卡和交换机是价格也很高，然而RoCEv2和iWARP仅需使用特殊的网卡就可以了，价格也相对便宜很多。 Infiniband，支持RDMA的新一代网络协议。 由于这是一种新的网络技术，因此需要支持该技术的NIC和交换机。RoCE，一个允许在以太网上执行RDMA的网络协议。 其较低的网络标头是以太网标头，其较高的网络标头（包括数据）是InfiniBand标头。 这支持在标准以太网基础设施（交换机）上使用RDMA。 只有网卡应该是特殊的，支持RoCE。iWARP，一个允许在TCP上执行RDMA的网络协议。 IB和RoCE中存在的功能在iWARP中不受支持。 这支持在标准以太网基础设施（交换机）上使用RDMA。 只有网卡应该是特殊的，并且支持iWARP（如果使用CPU卸载），否则所有iWARP堆栈都可以在SW中实现，并且丧失了大部分RDMA性能优势。 3.4 RDMA技术 传统上的RDMA技术设计内核封装多层网络协议并且涉及内核数据传输。RDMA通过专有的RDMA网卡RNIC，绕过内核直接从用户空间访问RDMA enabled NIC网卡。RDMA提供一个专有的verbs interface而不是传统的TCP/IP Socket interface。要使用RDMA首先要建立从RDMA到应用程序内存的数据路径 ，可以通过RDMA专有的verbs interface接口来建立这些数据路径，一旦数据路径建立后，就可以直接访问用户空间buffer。 3.5 RDMA整体系统架构图 上诉介绍的是RDMA整体框架架构图。从图中可以看出，RDMA在应用程序用户空间，提供了一系列verbs interface接口操作RDMA硬件。RDMA绕过内核直接从用户空间访问RDMA 网卡(RNIC)。RNIC网卡中包括Cached Page Table Entry，页表就是用来将虚拟页面映射到相应的物理页面。 3.6 RDMA技术详解RDMA 的工作过程如下: 1)当一个应用执行RDMA 读或写请求时，不执行任何数据复制.在不需要任何内核内存参与的条件下，RDMA 请求从运行在用户空间中的应用中发送到本地NIC( 网卡)。 2) NIC 读取缓冲的内容，并通过网络传送到远程NIC。 3) 在网络上传输的RDMA 信息包含目标虚拟地址、内存钥匙和数据本身.请求既可以完全在用户空间中处理(通过轮询用户级完成排列) ，又或者在应用一直睡眠到请求完成时的情况下通过系统中断处理.RDMA 操作使应用可以从一个远程应用的内存中读数据或向这个内存写数据。 4) 目标NIC 确认内存钥匙，直接将数据写人应用缓存中.用于操作的远程虚拟内存地址包含在RDMA 信息中。 3.7 RDMA操作细节 RDMA提供了基于消息队列的点对点通信，每个应用都可以直接获取自己的消息，无需操作系统和协议栈的介入。 消息服务建立在通信双方本端和远端应用之间创建的Channel-IO连接之上。当应用需要通信时，就会创建一条Channel连接，每条Channel的首尾端点是两对Queue Pairs（QP）。每对QP由Send Queue（SQ）和Receive Queue（RQ）构成，这些队列中管理着各种类型的消息。QP会被映射到应用的虚拟地址空间，使得应用直接通过它访问RNIC网卡。除了QP描述的两种基本队列之外，RDMA还提供一种队列Complete Queue（CQ），CQ用来知会用户WQ上的消息已经被处理完。 RDMA提供了一套软件传输接口，方便用户创建传输请求Work Request(WR），WR中描述了应用希望传输到Channel对端的消息内容，WR通知QP中的某个队列Work Queue(WQ)。在WQ中，用户的WR被转化为Work Queue Element（WQE）的格式，等待RNIC的异步调度解析，并从WQE指向的Buffer中拿到真正的消息发送到Channel对端。 3.7.1 RDAM单边操作 (RDMA READ)READ和WRITE是单边操作，只需要本端明确信息的源和目的地址，远端应用不必感知此次通信，数据的读或写都通过RDMA在RNIC与应用Buffer之间完成，再由远端RNIC封装成消息返回到本端。 对于单边操作，以存储网络环境下的存储为例，数据的流程如下： 首先A、B建立连接，QP已经创建并且初始化。 数据被存档在B的buffer地址VB，注意VB应该提前注册到B的RNIC (并且它是一个Memory Region) ，并拿到返回的local key，相当于RDMA操作这块buffer的权限。 B把数据地址VB，key封装到专用的报文传送到A，这相当于B把数据buffer的操作权交给了A。同时B在它的WQ中注册进一个WR，以用于接收数据传输的A返回的状态。 A在收到B的送过来的数据VB和R_key后，RNIC会把它们连同自身存储地址VA到封装RDMA READ请求，将这个消息请求发送给B，这个过程A、B两端不需要任何软件参与，就可以将B的数据存储到A的VA虚拟地址。 A在存储完成后，会向B返回整个数据传输的状态信息。 单边操作传输方式是RDMA与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用的参与其中，这种方式适用于批量数据传输。 3.7.2 RDMA 单边操作 (RDMA WRITE)对于单边操作，以存储网络环境下的存储为例，数据的流程如下： 首先A、B建立连接，QP已经创建并且初始化。 数据remote目标存储buffer地址VB，注意VB应该提前注册到B的RNIC(并且它是一个Memory Region)，并拿到返回的local key，相当于RDMA操作这块buffer的权限。 B把数据地址VB，key封装到专用的报文传送到A，这相当于B把数据buffer的操作权交给了A。同时B在它的WQ中注册进一个WR，以用于接收数据传输的A返回的状态。 A在收到B的送过来的数据VB和R_key后，RNIC会把它们连同自身发送地址VA到封装RDMA WRITE请求，这个过程A、B两端不需要任何软件参与，就可以将A的数据发送到B的VB虚拟地址。 A在发送数据完成后，会向B返回整个数据传输的状态信息。单边操作传输方式是RDMA与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用的参与其中，这种方式适用于批量数据传输。 3.7.3 RDMA 双边操作 (RDMA SEND/RECEIVE)RDMA中SEND/RECEIVE是双边操作，即必须要远端的应用感知参与才能完成收发。在实际中，SEND/RECEIVE多用于连接控制类报文，而数据报文多是通过READ/WRITE来完成的。对于双边操作为例，主机A向主机B(下面简称A、B)发送数据的流程如下： 首先，A和B都要创建并初始化好各自的QP，CQ A和B分别向自己的WQ中注册WQE，对于A，WQ=SQ，WQE描述指向一个等到被发送的数据；对于B，WQ=RQ，WQE描述指向一块用于存储数据的Buffer。 A的RNIC异步调度轮到A的WQE，解析到这是一个SEND消息，从Buffer中直接向B发出数据。数据流到达B的RNIC后，B的WQE被消耗，并把数据直接存储到WQE指向的存储位置。 AB通信完成后，A的CQ中会产生一个完成消息CQE表示发送完成。与此同时，B的CQ中也会产生一个完成消息表示接收完成。每个WQ中WQE的处理完成都会产生一个CQE。双边操作与传统网络的底层Buffer Pool类似，收发双方的参与过程并无差别，区别在零拷贝、Kernel Bypass，实际上对于RDMA，这是一种复杂的消息传输模式，多用于传输短的控制消息。 作者：MasterT-J来源：CSDN原文：https://blog.csdn.net/qq_21125183/article/details/80563463版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
  </entry>
  <entry>
    <title><![CDATA[测试页面]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[欢迎来到我的博客世界！！！！]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何在 CentOS 7 下安装 Nginx]]></title>
    <url>%2F2018%2F04%2F21%2F%E5%A6%82%E4%BD%95%E5%9C%A8-CentOS-7-%E4%B8%8B%E5%AE%89%E8%A3%85-Nginx%2F</url>
    <content type="text"><![CDATA[准备工作1）gcc 环境安装 安装 Nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，执行命令为： 1yum install gcc-c++ 2）zlib 库安装 zlib 库提供了很多种压缩和解压缩的方式， Nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。 1yum install -y zlib zlib-devel 3）pcre 库安装 PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。 1yum install -y pcre pcre-devel 4）openssl 安装 OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。 1yum install -y openssl openssl-devel 5）Nginx 源码下载 进入 Nginx 官网下载页面，下载最新稳定版本。 1wget -c http://nginx.org/download/nginx-1.14.0.tar.gz 如何安装 Nginx1）解压并进入目录 12tar -xzvf nginx-1.14.0.tar.gzcd nginx-1.14.0 2）默认配置 1./configure 3）编译安装 1make &amp;&amp; make install 如何启动 Nginx进入启动目录： 1cd /usr/local/nginx/sbin 1) 启动 1./nginx 2）停止 1234567./nginx -s quit此方式停止步骤是待 Nginx 进程处理任务完毕进行停止。./nginx -s stop此方式相当于先查出 Nginx进程 id 再使用 kill 命令强制杀掉进程 3）重启 1./nginx -s reload Nginx 测试启动 Nginx 之后，在浏览器输入该机器的 IP 地址，进入如下页面，即可说明 Nginx 成功安装并且启动成功了。]]></content>
      <categories>
        <category>Nginx</category>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDN 服务支持 HTTP/2 Server Push 特性]]></title>
    <url>%2F2018%2F04%2F18%2FHTTP-2-Server-push-%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[前言很长一段时间内，Nginx 并不支持 HTTP/2 的 Server Push 特性。好消息是最新版本的 Nginx 1.13.9 已支持该特性，详情请移步 Nginx 官方博客。这个特性的目的是让服务端将部分资源主动推送给客户端（浏览器），节约了客户端需要使用这些资源再次发送 GET 请求所消耗的时间。 又拍云在 Nginx 基础上，已在其 CDN 网络中全网支持 HTTP/2 的 Server Push 特性，这是既又拍云 CDN 全网支持 TLS 1.3 之后又一重要特性。我们将不遗余力的保持新特性的更新迭代，为全网用户带来更加快速的访问体验。 本文接下来将围绕如下 3 个方面来介绍 Server Push 特性： 何为 Server Push 如何使用 Server Push 如何验证 Server Push 是否生效 何为 Server PushServer Push 是 HTTP/2 规范中引入的一种新技术，也即服务端在没有被客户端明确的询问下，抢先的 “推送” 一些网站资源给客户端（浏览器）。该特性只要被正确的使用，可以达到很好的页面访问效果。为了更方便的理解，下文将进行对比分析： 未使用 Server Push 特性 图片来源：www.smashingmagazine.com WEB 浏览器访问 WEB 服务端遵循着请求-响应模式，也即 WEB 浏览器请求一个资源，WEB 服务器响应一个资源。以常规的网页为例，当请求一个 /index.html 后，WEB 服务端响应一个 /index.html 页面给 WEB 浏览器，此时 WEB 浏览器会去解析该 /index.html 页面，发现还需要去加载 JS、CSS、图片等资源，此时客户端会依次去请求这些资源。这无形当中影响了首屏渲染的时间，不利于页面快速加载和渲染。 已使用 Server Push 特性 图片来源：www.smashingmagazine.com 使用服务器推送（Server Push）技术之后，当 WEB 浏览器请求 /index.html 之后，WEB 服务端会直接将需要推送的资源一并发给 WEB 浏览器，而不需要 WEB 浏览器进行依次请求，这减少了 WEB 浏览器进行 GET 请求所消耗的时间。 如何使用 Server Push又拍云 CDN 支持 Server Push 特性可以通过如下两种方式来实现： 方式一利用 HTTP 的 Link 首部，这在 W3C Preload 工作草案中有详细描述。示例为： 1Link: &lt;/static/css/style.css&gt;; rel=preload; as=style; 其中，Link 首部中 as 是必选的，它告诉了浏览器推送的资源类型，例如 as=style 表明了推送的资源是一个样式表，除了样式表，您还可以推送其他的内容类型，详情参见支持的内容类型。如果需要进行多资源推送，可以进行如下设置： 1Link: &lt;/static/css/styles.css&gt;; rel=preload; as=style, &lt;/js/scripts.js&gt;; rel=preload; as=script, &lt;/img/logo.png&gt;; rel=preload; as=image 方式二在 CDN 控制台进行自定义 Server Push 配置，此时您无需在源站进行修改，例如： 匹配路径为： 1/index.html 推送资源为： 12/static/123.css/static/456.js 在 CDN 控制台的配置如截图所示： 其中【匹配路径】为必填项，【推送资源】为非必填项。 以上两种方式，需要注意如下事项： 如果源站已经通过 Link 首部来实现服务器推送，在 CDN 端的配置只需要配置【匹配路径】即可，无需配置【推送资源】选项；其中通过 Link 首部推送资源的方式， CDN 已经默认开启； 如果在 CDN 端进行自定义 Server Push 推送资源配置，则优先级会高于源站设置的 Link 首部； 无论何种实现方式，总的（包括 Link 首部和 CDN 自定义的方式）推送资源数量不超过 8 个。 如何验证 Server Push 是否生效1）通过 Google Chrome 浏览器进行测试 在 CDN 控制台进行了如下配置： 通过 Google Chrome 开发者工具进行抓包查看，推送的资源都被 Push 了，如截图所示： 查看 /index.html 资源响应头信息，并查看 x-upyun-h2-pushed 字段： 1234567891011121314151617age: 501691cache-control: max-age=691200content-encoding: brcontent-type: text/htmldate: Thu, 19 Apr 2018 05:32:26 GMTetag: W/&quot;86ef9cae8d9f9e1205b25357e78a149b&quot;expires: Sat, 21 Apr 2018 10:10:55 GMTlast-modified: Fri, 13 Apr 2018 10:10:45 GMTserver: marco/2.1set-cookie: UPYUNPUSH=582825323-1696419771-1484613131-3932011035; Max-Age=7200status: 200vary: Accept-Encodingvia: T.205.M, V.403-zj-fud-207, S.mix-sd-dst-035, T.40.M, V.mix-sd-dst-044, T.136.H, M.cun-sd-lyi1-136x-content-type: text/htmlx-request-id: 04dc2c7db2c509af1efc7d7252f0c2ce; 319efa6d981c0cb8dfb2b389368284f4x-source: U/200x-upyun-h2-pushed: /image/meinv1.jpg; /image/meinv2.jpg; /image/meinv3.jpg; /image/meinv4.jpg 其中，x-upyun-h2-pushed 字段内容为： 1x-upyun-h2-pushed: /image/meinv1.jpg; /image/meinv2.jpg; /image/meinv3.jpg; /image/meinv4.jpg 也可以说明所配置的推送资源被成功 Push 了。 2）通过 nghttp 工具进行测试 测试命令为： 1nghttp -ans https://server-push.upyun.club/index.html 测试结果如下： 123456id responseEnd requestStart process code size request path 13 +112.01ms +69us 111.94ms 200 167 /index.html 8 +1.31s * +56.96ms 1.25s 200 314K /image/meinv4.jpg 6 +2.17s * +56.95ms 2.11s 200 628K /image/meinv3.jpg 4 +2.34s * +56.94ms 2.28s 200 717K /image/meinv2.jpg 2 +2.42s * +56.91ms 2.36s 200 726K /image/meinv1.jpg 从测试结果中可以看出，被推推送的资源在 requestStart 栏左侧以星号标记了出来。 后语Server Push 作为 HTTP/2 最激动人心的特性之一，在性能提升方面是一大突破和挑战。又拍云紧紧跟随时代的步伐，寄希望为互联网用户提供更安全、更快的加速体验。与此同时，我们也很高兴成为国内首家推出 Server Push 功能的 CDN 厂商。我们期待您的测试和使用。更多反馈意见，请回复该博客，谢谢！ 参考文档：https://www.cloudflare.com/website-optimization/http2/serverpush/ https://www.smashingmagazine.com/2017/04/guide-http2-server-push/]]></content>
  </entry>
</search>
